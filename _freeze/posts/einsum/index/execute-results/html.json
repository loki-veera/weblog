{
  "hash": "90be8da341f242f86790345967f9a154",
  "result": {
    "markdown": "---\ntitle: Matrix Operations using EINSUM\nauthor: Lokesh Veeramachenei\ndate: '2023-04-09'\ncategories:\n  - linear algebra\n  - einsum\nformat:\n  html:\n    code-fold: false\nexecute:\n  enables: true\n---\n\n### Keywords:\neinsum, matrix multiplications, multidimensional linear algebra, multi-head attention.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n## Why should you use einsum?\nWorking on higher dimensional algebra like tensor multiplications/additions, they used to make picture perfect sense on paper. When trying to implement using APIs/frameworks like numpy/torch, I always end up in lots of confusion and frequently would have to check the resultant tensor shapes. Here comes einsum to the resuce for me. Einsum is easy to learn and all you need to know is, to understand the tensor shapes and how the resulting tensor should look like. Einsum is also able to perform multiple operations on resultant tensor like swap axes/transpose. Also most of the APIs be it numpy, torch, tensorflow has a builtin einsum function. As easy as it sounds, in my experience most of the machine learning enthusiasts have never come across this function and not everyone knows about this simple yet magical function. So lets directly jump to an example of 3D matrix multiplications, first we perform traditional way of multiplication and later look at the einsum way.\n\nLets say, we have to multiply two matrices of shapes (3, 4) and (3, 1, 9). For this, the only way is to bump the first matrix to 3D with shape (3, 4, 1) and multiply. Then resultant matrix would be of shape (3, 4, 9). This is usally done in torch as below\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nA = torch.randn(3, 4)\nB = torch.randn(3, 1, 9)\n# Bump A to 3D\nA = A.unsqueeze(-1)\nprint(f'matrix A Shape: {A.shape}')\n# Use torch matmul for matrix multiplication\nresultant = torch.matmul(A, B)\nprint(f'Resultant matrix shape: {resultant.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmatrix A Shape: torch.Size([3, 4, 1])\nResultant matrix shape: torch.Size([3, 4, 9])\n```\n:::\n:::\n\n\nSame multiplication in einsum is computed as\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nA = torch.randn(3, 4)\nB = torch.randn(3, 1, 9)\nes_resultant = torch.einsum('ij, iqr -> ijr', A, B)\nprint(f'Einsum resultant matrix shape: {es_resultant.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEinsum resultant matrix shape: torch.Size([3, 4, 9])\n```\n:::\n:::\n\n\nAs you see, it works without any need for dimensional bumping. For me this is very handy, when implementing the multihead attention in vanilla transformer as matrices are 4D. Further in this page, lets us go through how to use einsum and few of its applications in linear algebra.\n\n## How to use einsum?\nAs you see in the above cell, there are two major arguments given to the _torch.einsum_. The second argument is self explanatory as they are the matrices we want to operate on, here by referred as operands as in [1]. The important part of einsum is the first argument which is the equation. The equation is further divided into two parts, first part consisting of the indices of the matrices we are operating on. These are to the left side of the '->' and on the right side, we state the indices we want. Lets see a simple example of matrix transpose.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#----------------------\n# Transpose of a matrix\n#----------------------\nA = torch.randn(7, 5)\nprint(f'matrix A shape: {A.shape}')\nA_T = torch.einsum('ij->ji', A)\nprint(f'matrix A transpose shape: {A_T.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmatrix A shape: torch.Size([7, 5])\nmatrix A transpose shape: torch.Size([5, 7])\n```\n:::\n:::\n\n\nIn the above transpose example, we see that the left side of the equation consists of original indices order $(i,j)$ and to the right side, we specify the order of indices we need $(j, i)$.\n### Other matrix operations\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n#--------------------------\n# Sum of elements in matrix\n#--------------------------\nA = torch.tensor([[1, 2],[3, 4]])\nprint(f'matrix A:\\n{A}\\n')\nsum_A = torch.einsum('ij->', A)\nprint(f'Sum of elements: {sum_A}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmatrix A:\ntensor([[1, 2],\n        [3, 4]])\n\nSum of elements: 10\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#--------------------------------------\n# Sum across columns and rows in matrix\n#--------------------------------------\ncolumn_sum = torch.einsum('ij -> j', A)\nprint(f'Sum across columns: {column_sum}')\nrow_sum = torch.einsum('ij -> i', A)\nprint(f'Sum across rows: {row_sum}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSum across columns: tensor([4, 6])\nSum across rows: tensor([3, 7])\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#---------------------\n# Vector outer product\n#---------------------\nvec_A = torch.randn(3, 1)\nvec_B = torch.randn(4, 1)\nouter_AB = torch.einsum('ij, pj -> ip', vec_A, vec_B)\nprint(f'Outer product shape: {outer_AB.shape}')\n#---------------------------\n# Vector dot (inner) product\n#---------------------------\nvec_A = torch.tensor([[4, 2, 3]])\nvec_B = torch.tensor([[1, 1, 1]])\ninner_AB = torch.einsum('ij,ij->', vec_A, vec_B)\nprint(f'\\nDot product:\\n{inner_AB}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOuter product shape: torch.Size([3, 4])\n\nDot product:\n9\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#---------------------------\n# Matrix multiplication (2D)\n#---------------------------\nA = torch.randn(3, 7)\nB = torch.randn(7, 4)\nmatmul = torch.einsum('ij, jp -> ip', A, B)\nprint(f'Resultant shape: {matmul.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResultant shape: torch.Size([3, 4])\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n#---------------------------------------\n# Matrix Element wise (Hadamard) product\n#---------------------------------------\nA = torch.tensor([[1, 2], [3, 4]])\nB = torch.tensor([[1, 0], [0, 1]])\nprint(f'matrix A:\\n{A}\\n')\nprint(f'matrix B:\\n{B}\\n')\nelement_product = torch.einsum('ij, ij -> ij', A, B)\nprint(f'Elementwise product:\\n{element_product}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmatrix A:\ntensor([[1, 2],\n        [3, 4]])\n\nmatrix B:\ntensor([[1, 0],\n        [0, 1]])\n\nElementwise product:\ntensor([[1, 0],\n        [0, 4]])\n```\n:::\n:::\n\n\nI think, this gives better understanding on how to use einsum.\n\n## Practical usage:\nFurthermore lets see how einsum is used in real world examples. Take Transformers for example first proposed in [2], they took the field of deep learning in a whole new direction (in my opinion). Curcial part of transformers is multihead attention block as depicted in below image.\n![Multi head attention](./multihead_attention.jpeg \"Multi head attention. Image from [2]\")\n\nThe above image is taken from [2]. Let us implement this block using einsum.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# d_model = 512, d_k = 64, num_heads(h) = 8 as stated in paper [2].\n# Initialize Linear layers\nw_q  = torch.randn(64, 64)\nw_k  = torch.randn(64, 64)\nw_v  = torch.randn(64, 64)\nfinal_linear = torch.randn(512, 512)\nquery= key= value= torch.randn(16, 10, 512) #(batch_size, n_tokens, embedding_size)\n\n# split the data for multiple heads\nquery = query.reshape(16, 10, 8, 64) #(batch_size, n_tokens, num_heads, d_k)\nkey   = key.reshape(16, 10, 8, 64)\nvalue = value.reshape(16, 10, 8, 64)\n\n# Linear projection\nQ = torch.einsum('ijkl, lp -> ijkp', query, w_q)\nK = torch.einsum('ijkl, lp -> ijkp', key, w_k)\nV = torch.einsum('ijkl, lp -> ijkp', value, w_v)\n\n# Scaled dot product attention\nQ_K = torch.einsum('ijkl, ijrl -> ijkr', Q, K)\nQ_K /= torch.sqrt(torch.tensor(64))\nsoftmax_op = torch.nn.functional.softmax(Q_K, dim=-1)\nattention_weights = torch.einsum('ijkl, ijlr -> ikjr', softmax_op, V)\n\n# Concatenate multiple attention head outputs\nconcatenated_weights = attention_weights.reshape(16, 10, -1)\noutput = torch.einsum('ijk, kl -> ijl', concatenated_weights, final_linear)\nprint(output.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([16, 10, 512])\n```\n:::\n:::\n\n\nIn general, one can find lot of implementations of multihead attention using _torch.bmm( )_. Objective here is to show that this can be easily performed using einsum.\n\n### Exercise:\nObjective: Try to compute gaussian distribution (in both 1D and 2D) using einsum. In depth details of gaussian distribution can be found here in [3]. The below image depicts the guassian distribution formula taken from [3].\n![Multivariate Gaussian Distribution](./multigauss.jpeg \"Multivariate Gaussian Distribution\")\n\n## References:\n1. Pytorch contributors, torch.einsum, 2023. https://pytorch.org/docs/stable/generated/torch.einsum.html\n2. Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).\n3. Professor Ng, Anomaly detection - Stanford machine learning, 2019. http://www.holehouse.org/mlclass/15_Anomaly_Detection.html\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}