[
  {
    "objectID": "posts/condition_number_intro/index.html",
    "href": "posts/condition_number_intro/index.html",
    "title": "Condition Number Introduction",
    "section": "",
    "text": "condition number, matrix multiplications, linear algebra.\n\nimport torch\n\nIn this post, we will look into what is a condition number and how to calculate it. Along with that we will see an example of how the condition number affects the system of linear equations."
  },
  {
    "objectID": "posts/condition_number_intro/index.html#what-is-condition-number",
    "href": "posts/condition_number_intro/index.html#what-is-condition-number",
    "title": "Condition Number Introduction",
    "section": "What is condition number?",
    "text": "What is condition number?\nCondition number \\((\\kappa)\\) specifies the sensitivity of the matrix multiplication to small changes in the input. A matrix with a smaller condition number is said to be well conditioned and vice versa. The condition number of a matrix \\(M\\) is computed by the ratio of largest to smalled singular value of the matrix \\(M\\).\n\\[\n    \\kappa(M) = \\frac{\\sigma_{max}(M)}{\\sigma_{min}(M)}\n\\]\nThe conditional number of unitary matrices is 1. These matrices are important for orthogonalization of the neural networks.  Unitary matrcies: A matrix whose inverse equals to complex transpose. \\[\n    U^* = U^{-1} \\\\\n    UU^* = I\n\\]\n\ndef get_condition_number(matrix):\n    _, sigma, _ = torch.linalg.svd(matrix)\n    return max(sigma)/min(sigma)"
  },
  {
    "objectID": "posts/condition_number_intro/index.html#ill-conditioned-system",
    "href": "posts/condition_number_intro/index.html#ill-conditioned-system",
    "title": "Condition Number Introduction",
    "section": "Ill conditioned system",
    "text": "Ill conditioned system\nThe example for ill conditioned matrix given in [1] is\n\\[\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1.0001\n\\end{pmatrix}\\]\nNow lets see why this matrix is ill conditioned.  Let us consider a set of linear equations represented in the matrix form \\(Ax = b\\) with A being the above matrix.\nLet’s consider \\(b\\) matrix as\n\\[\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\\]\nThe system of equations is represented as \\[\n\\left(\\matrix{1 & 1 \\cr 1 & 1.0001}\\right)\\left(\\matrix{x \\cr y}\\right) =\n\\left(\\matrix{1 \\cr 1}\\right)\n\\] The solution to the system is \\(\\left(\\matrix{x \\cr y}\\right) = \\left(\\matrix{1 \\cr 0}\\right)\\)\n\nM = torch.Tensor([[1, 1], [1, 1.0001]])\nb = torch.Tensor([[1], [1]])\nresult = torch.linalg.inv(M)@b\nprint(f\"The solution to the system with above conditions is: \\n {result}\")\n\nThe solution to the system with above conditions is: \n tensor([[1.],\n        [0.]])\n\n\nLet’s say because of numerical error \\(b\\) matrix is slightly changed as\n\\[\\begin{pmatrix}\n1 \\\\\n1.0002\n\\end{pmatrix}\\]\nNow the system of equations are: \\[\n\\left(\\matrix{1 & 1 \\cr 1 & 1.0001}\\right)\\left(\\matrix{x \\cr y}\\right) =\n\\left(\\matrix{1 \\cr 1.0002}\\right)\n\\] The solution to the system is \\(\\left(\\matrix{x \\cr y}\\right) = \\left(\\matrix{-1 \\cr 2}\\right)\\)\n\nx = torch.Tensor([[1], [1.0002]])\nresult = torch.linalg.inv(M)@x\nprint(f\"The solution to the system with slight change in b is: \\n {result}\")\n\nThe solution to the system with slight change in b is: \n tensor([[-1.],\n        [ 2.]])\n\n\n\ncondition_number = get_condition_number(M)\nprint(f\"Condition number of the matrix M is {condition_number}\")\n\nCondition number of the matrix M is 39949.36328125\n\n\nAs one can see that with the change in one of the element (\\(4^{th}\\) decimal), changed the solution in units place. So that is why this matrix is ill conditioned with large condition number of 39949.363."
  },
  {
    "objectID": "posts/condition_number_intro/index.html#well-conditioned-system",
    "href": "posts/condition_number_intro/index.html#well-conditioned-system",
    "title": "Condition Number Introduction",
    "section": "Well conditioned system",
    "text": "Well conditioned system\nLet’s look at how well conditioned matrix behaves to small changes. An example of such matrix is\n\\[\\begin{pmatrix}\n3 & 2 \\\\\n1 & 4\n\\end{pmatrix}\\]\nwith the condition number 2.62.\n\nM_well = torch.Tensor([[3, 2], [1, 4]])\nprint(get_condition_number(M_well))\n\ntensor(2.6180)\n\n\nWith \\(b\\) matrix as\n\\[\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\\]\nThe solution to the system is\n\nresult =  torch.linalg.inv(M_well)@b\nprint(result)\n\ntensor([[0.2000],\n        [0.2000]])\n\n\nWith \\(b\\) matrix slightly changed as\n\\[\\begin{pmatrix}\n1 \\\\\n1.0002\n\\end{pmatrix}\\]\nThe solution to the system is\n\nresult = torch.linalg.inv(M_well)@x\nprint(result)\n\ntensor([[0.2000],\n        [0.2001]])\n\n\nOne can see that, with the change in \\(4^{th}\\) decimal, the solution to the system is only changed in \\(3^{rd}\\) decimal.\nIn the next post, we will see why this is important in training neural networks and how condition number affects the training regime."
  },
  {
    "objectID": "posts/condition_number_intro/index.html#references",
    "href": "posts/condition_number_intro/index.html#references",
    "title": "Condition Number Introduction",
    "section": "References:",
    "text": "References:\n\nStrang, Gilbert. Linear algebra and its applications. Belmont, CA: Thomson, Brooks/Cole, 2006."
  },
  {
    "objectID": "posts/einsum/index.html",
    "href": "posts/einsum/index.html",
    "title": "Matrix Operations using EINSUM",
    "section": "",
    "text": "einsum, matrix multiplications, multidimensional linear algebra, multi-head attention.\n\nimport torch"
  },
  {
    "objectID": "posts/einsum/index.html#why-should-you-use-einsum",
    "href": "posts/einsum/index.html#why-should-you-use-einsum",
    "title": "Matrix Operations using EINSUM",
    "section": "Why should you use einsum?",
    "text": "Why should you use einsum?\nWorking on higher dimensional algebra like tensor multiplications/additions, they used to make picture perfect sense on paper. When trying to implement using APIs/frameworks like numpy/torch, I always end up in lots of confusion and frequently would have to check the resultant tensor shapes. Here comes einsum to the resuce for me. Einsum is easy to learn and all you need to know is, to understand the tensor shapes and how the resulting tensor should look like. Einsum is also able to perform multiple operations on resultant tensor like swap axes/transpose. Also most of the APIs be it numpy, torch, tensorflow has a builtin einsum function. As easy as it sounds, in my experience most of the machine learning enthusiasts have never come across this function and not everyone knows about this simple yet magical function. So lets directly jump to an example of 3D matrix multiplications, first we perform traditional way of multiplication and later look at the einsum way.\nLets say, we have to multiply two matrices of shapes (3, 4) and (3, 1, 9). For this, the only way is to bump the first matrix to 3D with shape (3, 4, 1) and multiply. Then resultant matrix would be of shape (3, 4, 9). This is usally done in torch as below\n\nA = torch.randn(3, 4)\nB = torch.randn(3, 1, 9)\n# Bump A to 3D\nA = A.unsqueeze(-1)\nprint(f'matrix A Shape: {A.shape}')\n# Use torch matmul for matrix multiplication\nresultant = torch.matmul(A, B)\nprint(f'Resultant matrix shape: {resultant.shape}')\n\nmatrix A Shape: torch.Size([3, 4, 1])\nResultant matrix shape: torch.Size([3, 4, 9])\n\n\nSame multiplication in einsum is computed as\n\nA = torch.randn(3, 4)\nB = torch.randn(3, 1, 9)\nes_resultant = torch.einsum('ij, iqr -&gt; ijr', A, B)\nprint(f'Einsum resultant matrix shape: {es_resultant.shape}')\n\nEinsum resultant matrix shape: torch.Size([3, 4, 9])\n\n\nAs you see, it works without any need for dimensional bumping. For me this is very handy, when implementing the multihead attention in vanilla transformer as matrices are 4D. Further in this page, lets us go through how to use einsum and few of its applications in linear algebra."
  },
  {
    "objectID": "posts/einsum/index.html#how-to-use-einsum",
    "href": "posts/einsum/index.html#how-to-use-einsum",
    "title": "Matrix Operations using EINSUM",
    "section": "How to use einsum?",
    "text": "How to use einsum?\nAs you see in the above cell, there are two major arguments given to the torch.einsum. The second argument is self explanatory as they are the matrices we want to operate on, here by referred as operands as in [1]. The important part of einsum is the first argument which is the equation. The equation is further divided into two parts, first part consisting of the indices of the matrices we are operating on. These are to the left side of the ‘-&gt;’ and on the right side, we state the indices we want. Lets see a simple example of matrix transpose.\n\n#----------------------\n# Transpose of a matrix\n#----------------------\nA = torch.randn(7, 5)\nprint(f'matrix A shape: {A.shape}')\nA_T = torch.einsum('ij-&gt;ji', A)\nprint(f'matrix A transpose shape: {A_T.shape}')\n\nmatrix A shape: torch.Size([7, 5])\nmatrix A transpose shape: torch.Size([5, 7])\n\n\nIn the above transpose example, we see that the left side of the equation consists of original indices order \\((i,j)\\) and to the right side, we specify the order of indices we need \\((j, i)\\).\n\nOther matrix operations\n\n#--------------------------\n# Sum of elements in matrix\n#--------------------------\nA = torch.tensor([[1, 2],[3, 4]])\nprint(f'matrix A:\\n{A}\\n')\nsum_A = torch.einsum('ij-&gt;', A)\nprint(f'Sum of elements: {sum_A}')\n\nmatrix A:\ntensor([[1, 2],\n        [3, 4]])\n\nSum of elements: 10\n\n\n\n#--------------------------------------\n# Sum across columns and rows in matrix\n#--------------------------------------\ncolumn_sum = torch.einsum('ij -&gt; j', A)\nprint(f'Sum across columns: {column_sum}')\nrow_sum = torch.einsum('ij -&gt; i', A)\nprint(f'Sum across rows: {row_sum}')\n\nSum across columns: tensor([4, 6])\nSum across rows: tensor([3, 7])\n\n\n\n#---------------------\n# Vector outer product\n#---------------------\nvec_A = torch.randn(3, 1)\nvec_B = torch.randn(4, 1)\nouter_AB = torch.einsum('ij, pj -&gt; ip', vec_A, vec_B)\nprint(f'Outer product shape: {outer_AB.shape}')\n#---------------------------\n# Vector dot (inner) product\n#---------------------------\nvec_A = torch.tensor([[4, 2, 3]])\nvec_B = torch.tensor([[1, 1, 1]])\ninner_AB = torch.einsum('ij,ij-&gt;', vec_A, vec_B)\nprint(f'\\nDot product:\\n{inner_AB}')\n\nOuter product shape: torch.Size([3, 4])\n\nDot product:\n9\n\n\n\n#---------------------------\n# Matrix multiplication (2D)\n#---------------------------\nA = torch.randn(3, 7)\nB = torch.randn(7, 4)\nmatmul = torch.einsum('ij, jp -&gt; ip', A, B)\nprint(f'Resultant shape: {matmul.shape}')\n\nResultant shape: torch.Size([3, 4])\n\n\n\n#---------------------------------------\n# Matrix Element wise (Hadamard) product\n#---------------------------------------\nA = torch.tensor([[1, 2], [3, 4]])\nB = torch.tensor([[1, 0], [0, 1]])\nprint(f'matrix A:\\n{A}\\n')\nprint(f'matrix B:\\n{B}\\n')\nelement_product = torch.einsum('ij, ij -&gt; ij', A, B)\nprint(f'Elementwise product:\\n{element_product}')\n\nmatrix A:\ntensor([[1, 2],\n        [3, 4]])\n\nmatrix B:\ntensor([[1, 0],\n        [0, 1]])\n\nElementwise product:\ntensor([[1, 0],\n        [0, 4]])\n\n\nI think, this gives better understanding on how to use einsum."
  },
  {
    "objectID": "posts/einsum/index.html#practical-usage",
    "href": "posts/einsum/index.html#practical-usage",
    "title": "Matrix Operations using EINSUM",
    "section": "Practical usage:",
    "text": "Practical usage:\nFurthermore lets see how einsum is used in real world examples. Take Transformers for example first proposed in [2], they took the field of deep learning in a whole new direction (in my opinion). Curcial part of transformers is multihead attention block as depicted in below image. \nThe above image is taken from [2]. Let us implement this block using einsum.\n\n# d_model = 512, d_k = 64, num_heads(h) = 8 as stated in paper [2].\n# Initialize Linear layers\nw_q  = torch.randn(64, 64)\nw_k  = torch.randn(64, 64)\nw_v  = torch.randn(64, 64)\nfinal_linear = torch.randn(512, 512)\nquery= key= value= torch.randn(16, 10, 512) #(batch_size, n_tokens, embedding_size)\n\n# split the data for multiple heads\nquery = query.reshape(16, 10, 8, 64) #(batch_size, n_tokens, num_heads, d_k)\nkey   = key.reshape(16, 10, 8, 64)\nvalue = value.reshape(16, 10, 8, 64)\n\n# Linear projection\nQ = torch.einsum('ijkl, lp -&gt; ijkp', query, w_q)\nK = torch.einsum('ijkl, lp -&gt; ijkp', key, w_k)\nV = torch.einsum('ijkl, lp -&gt; ijkp', value, w_v)\n\n# Scaled dot product attention\nQ_K = torch.einsum('ijkl, ijrl -&gt; ijkr', Q, K)\nQ_K /= torch.sqrt(torch.tensor(64))\nsoftmax_op = torch.nn.functional.softmax(Q_K, dim=-1)\nattention_weights = torch.einsum('ijkl, ijlr -&gt; ikjr', softmax_op, V)\n\n# Concatenate multiple attention head outputs\nconcatenated_weights = attention_weights.reshape(16, 10, -1)\noutput = torch.einsum('ijk, kl -&gt; ijl', concatenated_weights, final_linear)\nprint(output.shape)\n\ntorch.Size([16, 10, 512])\n\n\nIn general, one can find lot of implementations of multihead attention using torch.bmm( ). Objective here is to show that this can be easily performed using einsum.\n\nExercise:\nObjective: Try to compute gaussian distribution (in both 1D and 2D) using einsum. In depth details of gaussian distribution can be found here in [3]. The below image depicts the guassian distribution formula taken from [3]."
  },
  {
    "objectID": "posts/einsum/index.html#references",
    "href": "posts/einsum/index.html#references",
    "title": "Matrix Operations using EINSUM",
    "section": "References:",
    "text": "References:\n\nPytorch contributors, torch.einsum, 2023. https://pytorch.org/docs/stable/generated/torch.einsum.html\nVaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).\nProfessor Ng, Anomaly detection - Stanford machine learning, 2019. http://www.holehouse.org/mlclass/15_Anomaly_Detection.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "weblog",
    "section": "",
    "text": "Condition Number Introduction\n\n\n\n\n\n\n\nlinear algebra\n\n\ncondition number\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nLokesh Veeramacheneni\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Operations using EINSUM\n\n\n\n\n\n\n\nlinear algebra\n\n\neinsum\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2023\n\n\nLokesh Veeramachenei\n\n\n\n\n\n\nNo matching items"
  }
]